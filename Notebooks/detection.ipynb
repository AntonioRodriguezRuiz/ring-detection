{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ring Detection\n",
    "\n",
    "To detect the rings generated in the previous steps, we use a clustering algorithm, which will give us an estimate of their centers and radii\n",
    "\n",
    "The data we want to extract from the csv files are the PointsSet class instaces, to later calculate error, and just a list of all points, to make the fuzzy c-means algorithm. We will use this algorithm because it allows us to assign a membership degree for each cluster to a point, instead of only assigning it to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_LOCATION = \"../dataset\" # Specifies where the dataset is located\n",
    "OUTPUT_DIRECTORY = \"../results\" # Specifies where the results of the ring detection will be saved\n",
    "FUZZINESS_RANGE = (1.05, 2.0)\n",
    "ATTEMPTS = 40 # Number of times we will run the algorithm per csv file\n",
    "MAX_ITERATIONS = 100 # Maximum number of iterations to find the center points\n",
    "MEMBERSHIP_THRESSHOLD = 0.3 # Minimum membership degree for a point to be considered belonging to a cluster (0.0 - 1.0), used to calculate radius\n",
    "DATA_KNOWN = True # Whether the csv files contain the real centers and radii\n",
    "NUM_CIRCLES = 3 # If no DATA_KNOWN, how many centroids to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, os, datetime, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointsSet Class\n",
    "\n",
    "We will use this class again here to help parse the csv and reconstruct the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointsSet:\n",
    "    def __init__(self, points, center, radius, circ_no):\n",
    "        self.points = points\n",
    "        self.center = center\n",
    "        self.radius = radius\n",
    "        self.circ_no = circ_no\n",
    "\n",
    "    def parse(points, center, radius, circ_no):\n",
    "        points = points\n",
    "        center = center if not math.isnan(circ_no) else None\n",
    "        radius = radius if not math.isnan(circ_no) else None\n",
    "        circ_no = int(circ_no) if not math.isnan(circ_no) else None\n",
    "        return PointsSet(points, center, radius, circ_no)\n",
    "\n",
    "    def add_point(self, point):\n",
    "        self.points.append(point)\n",
    "\n",
    "    def is_noise(self):\n",
    "        return self.circ_no is None\n",
    "\n",
    "    def unpack(self):\n",
    "        if self.is_noise():\n",
    "            return [[p[0], p[1], None, None, None] for p in self.points]\n",
    "        else:\n",
    "            return [[p[0], p[1], self.center[0], self.center[1], self.radius, self.circ_no] for p in self.points]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.is_noise():\n",
    "            return f\"{len(self.points)} of Noise\"\n",
    "        else:\n",
    "            return f\"Circunference {self.circ_no} has {len(self.points)} points and center in {self.center}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "The first step to represent the data is to extract it from the csv files. This time we have to take care of NaN values since comparing it to others will result on each noise point to be on its own set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_point_sets(df):\n",
    "    data = []\n",
    "    if DATA_KNOWN:\n",
    "        for _, row in df.iterrows():\n",
    "            existing_ps = next(filter(lambda ps: ps.circ_no==row.circ_no if not math.isnan(row.circ_no) else ps.circ_no is None, data), None)\n",
    "            if existing_ps is not None:\n",
    "                existing_ps.add_point((row.point_x, row.point_y))\n",
    "            else:\n",
    "                data.append(PointsSet.parse([(row.point_x, row.point_y)], (row.center_x, row.center_y), row.radius, row.circ_no))\n",
    "        \n",
    "        noise = [ps.points for ps in list(filter(lambda x : x.is_noise() , data))]\n",
    "        rings = sorted(list(filter(lambda x : not x.is_noise() , data)), key=lambda x: x.circ_no)\n",
    "    else:\n",
    "        for _, row in df.iterrows():\n",
    "            data.append(PointsSet.parse([(row.point_x, row.point_y)], math.nan, math.nan, math.nan))\n",
    "        rings = None\n",
    "        noise = None\n",
    "\n",
    "    points = []\n",
    "    for points_set in data:\n",
    "        points.extend(points_set.points)\n",
    "\n",
    "    return (points, rings, noise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy C-Means Algorithm\n",
    "\n",
    "We define here the fuzzy c-means algorithm, and starting with a random ammount of centers, we will make a certain ammount of iterations until the centers do not change.\n",
    "\n",
    "For this specific problem, if a center is found to have no points assigned to its cluster, we relocate it randomly, because we know we have exactly k rings, and we wish to find k centers\n",
    "\n",
    "Apart from the parameters we take in the k-means algorithm, we also take \"m\" as a parameter. This value defines how much sharing of data points we want to allow.\n",
    "\n",
    "We will run this algorithm until the center points do not change or when we reach se specified ammount of maximum iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_iteration(centers, points, m):\n",
    "    # m is the fuzziness parameter, usually set to 2\n",
    "    fuzziness = 2 / (m - 1)\n",
    "    n_clusters = len(centers)\n",
    "    n_points = len(points)\n",
    "    membership_matrix = np.zeros((n_points, n_clusters))\n",
    "\n",
    "    np_centers = np.array(centers)\n",
    "    np_points = np.array(points)\n",
    "    \n",
    "    # Legacy Loop\n",
    "    #for i in range(n_points):\n",
    "    #    for j in range(n_clusters):\n",
    "    #        distance = np.sqrt((centers[j][0]-points[i][0])**2 + (centers[j][1]-points[i][1])**2)\n",
    "    #        membership_matrix[i][j] = 1 / ((sum([(distance/np.sqrt((centers[k][0]-points[i][0])**2 + (centers[k][1]-points[i][1])**2))**(fuzziness) for k in range(n_clusters)])))\n",
    "\n",
    "    # Optimized calcuation\n",
    "    distances = np.sqrt(np.sum((np_centers[:, np.newaxis, :] - np_points)**2, axis=2))\n",
    "    distances = distances.T\n",
    "    membership_matrix = 1 / ((distances[:, :, np.newaxis]/distances[:, np.newaxis, :]) ** (fuzziness)).sum(axis=2)\n",
    "\n",
    "    # update the cluster centroids based on the membership degrees\n",
    "    new_centers = [(0,0) for _ in range(n_clusters)]\n",
    "    for j in range(n_clusters):\n",
    "        new_centers[j] = tuple(np.average(points, axis=0, weights=membership_matrix[:, j]**m))\n",
    "    return new_centers, membership_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alg_perform(k, points):\n",
    "    old_centers = [(random.uniform(0.0, 100.0), random.uniform(0.0, 100.0)) for _ in range(k)]\n",
    "    fuzziness = random.uniform(*FUZZINESS_RANGE)\n",
    "    new_centers, membership_matrix = alg_iteration(old_centers, points, fuzziness)\n",
    "    count = 1\n",
    "    while(count<=MAX_ITERATIONS and old_centers!=new_centers):\n",
    "        old_centers = new_centers\n",
    "        new_centers, membership_matrix = alg_iteration(old_centers, points, fuzziness)\n",
    "        count +=1\n",
    "    return new_centers, membership_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' K-Means version\\ndef alg_iteration(centers, points):\\n    new_centers = [(0,0) for _ in range(len(centers))]\\n    clusters_points_mapping = {}\\n    [clusters_points_mapping.setdefault(i, []) for i in range(len(centers))]\\n    for p in points:\\n        best_distance_to_center = None\\n        for i in range(len(centers)):\\n            distance_to_center = math.sqrt((centers[i][0]-p[0])**2 + (centers[i][1]-p[1])**2)\\n            nearest_center = i if best_distance_to_center is None or distance_to_center < best_distance_to_center else nearest_center\\n            best_distance_to_center = distance_to_center if best_distance_to_center is None else min(best_distance_to_center, distance_to_center)\\n        clusters_points_mapping[nearest_center].append(p)\\n    \\n    for index in clusters_points_mapping:\\n        if(len(clusters_points_mapping[index])==0):\\n            new_centers[index] = (random.uniform(0.0, 100.0), random.uniform(0.0, 100.0))\\n        else:\\n            new_centers[index] = (np.median([p[0] for p in clusters_points_mapping[index]]), np.median([p[1] for p in clusters_points_mapping[index]]))\\n\\n    return new_centers\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' K-Means version\n",
    "def alg_iteration(centers, points):\n",
    "    new_centers = [(0,0) for _ in range(len(centers))]\n",
    "    clusters_points_mapping = {}\n",
    "    [clusters_points_mapping.setdefault(i, []) for i in range(len(centers))]\n",
    "    for p in points:\n",
    "        best_distance_to_center = None\n",
    "        for i in range(len(centers)):\n",
    "            distance_to_center = math.sqrt((centers[i][0]-p[0])**2 + (centers[i][1]-p[1])**2)\n",
    "            nearest_center = i if best_distance_to_center is None or distance_to_center < best_distance_to_center else nearest_center\n",
    "            best_distance_to_center = distance_to_center if best_distance_to_center is None else min(best_distance_to_center, distance_to_center)\n",
    "        clusters_points_mapping[nearest_center].append(p)\n",
    "    \n",
    "    for index in clusters_points_mapping:\n",
    "        if(len(clusters_points_mapping[index])==0):\n",
    "            new_centers[index] = (random.uniform(0.0, 100.0), random.uniform(0.0, 100.0))\n",
    "        else:\n",
    "            new_centers[index] = (np.median([p[0] for p in clusters_points_mapping[index]]), np.median([p[1] for p in clusters_points_mapping[index]]))\n",
    "\n",
    "    return new_centers\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radii Estimation\n",
    "\n",
    "Now that we have calculated the centers, we need to estimate the radii of the circunferences. For that we will take into account the membership degree and thresshold defined to consider a point top be part of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_radii(center, points, center_membership_matrix):\n",
    "    member_weights = center_membership_matrix[center_membership_matrix >= MEMBERSHIP_THRESSHOLD]\n",
    "    member_points = np.asarray(points)[center_membership_matrix >= MEMBERSHIP_THRESSHOLD]\n",
    "    return np.average([np.sqrt((center[0]-p[0])**2 + (center[1]-p[1])**2) for p in member_points], weights=member_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error\n",
    "\n",
    "The plan is to execute the algorithm more than once per sample, and then choose the best estimation out of all of them. For that, we need to calculate the error, meaning the \"distance\" between the current estimated centers and the actual centers themselves.\n",
    "\n",
    "We will also take into account the radii of the rings, however these will be considered on a different level, because we can have good centers and bad radii (circle inside of another), or bad centers but good radii.\n",
    "\n",
    "So that is why we will give 2 error values, one for the distance between the expected center and estimated one, and one for the difference between the expected and predicted radii.\n",
    " \n",
    "In order to calculate both error values, we need to first relate each of the predicted centers with each real center, for that we have to select 3 pairs such as the total distance is minimal, and then we can use that same distance for the error calculation. To find these 3 pairs we will use a matrix which will contain the cost of a certain pair (distance in this case), then we can solve it as a linear optimization problem finding the minimum combination of values in the matrix choosing only 1 value per row and column. We can do this last part with the linear_sum_assignment() function of scipy.\n",
    "\n",
    "The error we will return for both estimations will be an average of the errors for each circunference, in a value from 0.0 to 1.0:\n",
    "- In the case of centers: 1.0 being sqrt(100² + 100²) (sqrt(2000)) the distance between the estimate and the real center and 0.0 being exact\n",
    "- In the case of radii: 1.0 being the real radius value the difference between the size of the real radius and estimated one, and 0.0 being exact\n",
    "\n",
    "We have, however, to give one single value of error for us to make the comparison between different estimations for the same cloud of points, we will then give more value to the centers than to the radii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(centers, rings_centers):\n",
    "    cost_matrix = np.zeros((len(centers), len(rings_centers)))\n",
    "    \n",
    "    for i in range(len(centers)):\n",
    "        for j in range(len(rings_centers)):\n",
    "            cost_matrix[i][j] = np.sqrt((centers[i][0] - rings_centers[j][0])**2 + (centers[i][1] - rings_centers[j][1])**2)\n",
    "    center_ind, ring_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    return [[int(center_ind[i]), int(ring_ind[i])] for i in range(len(center_ind))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_error(pairs, centers, rings):\n",
    "    avg_offset = np.mean([np.sqrt((centers[p[0]][0] - rings[p[1]].center[0])**2 + (centers[p[0]][1] - rings[p[1]].center[1])**2) for p in pairs])\n",
    "    return avg_offset / np.sqrt(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_radii_error(pairs, centers, rings, points, membership_matrix):\n",
    "    return np.mean([abs(estimate_radii(centers[p[0]], points, membership_matrix[:, p[0]]) - rings[p[1]].radius)/rings[p[1]].radius for p in pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(centers_error, radii_error):\n",
    "    return centers_error * 0.8 + radii_error * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_error(centers, rings, points, membership_matrix):\n",
    "    pairs = find_pairs(centers, [r.center for r in rings])\n",
    "    return get_centers_error(pairs, centers, rings)*0.8 + get_radii_error(pairs, centers, rings, points, membership_matrix)*0.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop\n",
    "\n",
    "With all the previous functions, now its time to run the algorithm. But before that we will define and create the data structure to save all the dataset information and predicted data, so that we only have to extract information from one json file when studying the results. \n",
    "\n",
    "Initially, we do not do this with the dataset because we prefer it to be more readable and flexible, which a folder based structure provides. However when it comes to reading results, if we do not have everything we need in one place, we might risk losing information later down the road, for example, if the dataset changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b6c1c5e25140c9add0a75688a370af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting clouds of type clean:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95669125529b4730a3e983f8fab3fb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting clouds of type extends:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bd38f8943f4f0daf97925839dd4386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting clouds of type collides:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "'''\n",
    "{\n",
    "    set_type: \n",
    "    {\n",
    "        filename:\n",
    "        {\n",
    "            circs_num = int,\n",
    "            circunferences:\n",
    "            {\n",
    "                cir_no:\n",
    "                {\n",
    "                    points: [],\n",
    "                    center: [],\n",
    "                    radius: []\n",
    "                }\n",
    "            }\n",
    "            noise: [],\n",
    "            pairs: [],\n",
    "            predicted_centers: [],\n",
    "            predicted_radii: [],\n",
    "            centers_error: float,\n",
    "            radii_error: float,\n",
    "            tot_error: float\n",
    "        }\n",
    "    }  \n",
    "}\n",
    "'''\n",
    "\n",
    "for set_type in [\"clean\", \"extends\", \"collides\"]:\n",
    "    results[set_type] = {}\n",
    "\n",
    "    for filename in tqdm(os.listdir(f\"{DATASET_LOCATION}/{set_type}\"), desc=f\"Predicting clouds of type {set_type}\", leave=True):\n",
    "        if filename.endswith(\".csv\"): \n",
    "            df = pd.read_csv(f\"{DATASET_LOCATION}/{set_type}/{filename}\",header=0, sep=\";\")\n",
    "            points, rings, noise = extract_point_sets(df)\n",
    "            if DATA_KNOWN:\n",
    "                predicted_centers, membership_matrix = min([alg_perform(len(rings), points) for _ in tqdm(range(ATTEMPTS), desc=f\"Predicting {filename}\", leave=False)], key=lambda c: get_total_error(c[0], rings, points, c[1]))\n",
    "\n",
    "            else:\n",
    "                predicted_centers, membership_matrix = alg_perform(NUM_CIRCLES, points)\n",
    "\n",
    "            pairs = find_pairs(predicted_centers, [r.center for r in rings]) if DATA_KNOWN else None\n",
    "            centers_error = get_centers_error(pairs, predicted_centers, rings) if DATA_KNOWN else None\n",
    "            radii_error = get_radii_error(pairs, predicted_centers, rings, points, membership_matrix) if DATA_KNOWN else None\n",
    "            if DATA_KNOWN:\n",
    "                results[set_type][filename] =   {\n",
    "                                                \"circs_num\": len(rings),\n",
    "                                                \"circunferences\":\n",
    "                                                {\n",
    "                                                    f\"{r.circ_no}\":\n",
    "                                                    {\n",
    "                                                        \"points\": r.points,\n",
    "                                                        \"center\": r.center,\n",
    "                                                        \"radius\": r.radius\n",
    "                                                    }\n",
    "                                                    for r in rings\n",
    "                                                },\n",
    "                                                \"noise\": noise,\n",
    "                                                \"pairs\": pairs, \n",
    "                                                \"predicted_centers\": predicted_centers, \n",
    "                                                \"predicted_radii\": [estimate_radii(predicted_centers[p[0]], points, membership_matrix[:, p[0]]) for p in pairs],\n",
    "                                                \"membership_matrix\" : membership_matrix.tolist(),\n",
    "                                                \"centers_error\": centers_error,\n",
    "                                                \"radii_error\": radii_error,\n",
    "                                                \"tot_error\": get_error(centers_error, radii_error)\n",
    "                                                }\n",
    "            else:\n",
    "                results[set_type][filename] =   {\n",
    "                                                \"circs_num\": NUM_CIRCLES,\n",
    "                                                \"points\": points,\n",
    "                                                \"predicted_centers\": predicted_centers, \n",
    "                                                \"predicted_radii\": [estimate_radii(predicted_centers[i], points, membership_matrix[:, i]) for i in range(len(predicted_centers))],\n",
    "                                                \"membership_matrix\" : membership_matrix.tolist()\n",
    "                                                }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "\n",
    "We finally save the results to a json, in a folder specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_moment = datetime.datetime.now()\n",
    "if not os.path.exists(f\"{OUTPUT_DIRECTORY}\"):\n",
    "            os.makedirs(f\"{OUTPUT_DIRECTORY}\")\n",
    "with open(f\"{OUTPUT_DIRECTORY}/results_{current_moment}.json\", 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2446266b7054609a6dae69b456e88a59e71437ec47e5b51a68b438a99636ed8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
